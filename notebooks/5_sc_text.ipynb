{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "533b6a93",
   "metadata": {},
   "source": [
    "# 5 · Smart Connections · Embeddings de texto\n",
    "Pipeline de texto para Smart Connections:\n",
    "1) Carga `data/matchings_products.jsonl` y pares gold de `data/matchings_pairs.jsonl` (creados en el paso 4).\n",
    "2) Calcula embeddings de texto con MiniLM (fallback hash BOW si falta el modelo).\n",
    "3) Genera 1 distractor por par gold.\n",
    "4) Calcula score=coseno y similarity.\n",
    "5) Guarda `data/matchings_text.npy`, `data/matchings_index.json` y sobrescribe `data/matchings_pairs.jsonl` con scores.\n",
    "6) Imprime medias y el caso Garmin para depurar.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fc6530b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando sentence-transformers/all-MiniLM-L6-v2\n",
      "Embeddings guardados: /Users/marc/Documents/Projectes/tfm-product-matching/data/matchings_text.npy\n",
      "Pares escritos en /Users/marc/Documents/Projectes/tfm-product-matching/data/matchings_pairs.jsonl\n",
      "Mean score label=1: 0.6095 | label=0: 0.2098\n",
      "Caso Garmin:\n",
      "pair 0 label=1 score=0.7750 comp=831121\n",
      "pair 2000 label=0 score=0.1524 comp=178374\n",
      "pair 1 label=1 score=0.7145 comp=1143212\n",
      "pair 2001 label=0 score=0.1943 comp=574333\n",
      "Listo. Recarga la API con ?nocache=1\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "import json, math, random\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Any\n",
    "import numpy as np\n",
    "\n",
    "try:\n",
    "    PROJECT_ROOT = Path(__file__).resolve().parents[1]\n",
    "except NameError:\n",
    "    PROJECT_ROOT = Path.cwd().resolve().parents[0]\n",
    "    if not (PROJECT_ROOT / \"data\").exists() and len(PROJECT_ROOT.parents) > 0:\n",
    "        PROJECT_ROOT = PROJECT_ROOT.parent\n",
    "\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "PROD_PATH = DATA_DIR / \"matchings_products.jsonl\"\n",
    "PAIR_PATH = DATA_DIR / \"matchings_pairs.jsonl\"\n",
    "EMB_PATH = DATA_DIR / \"matchings_text.npy\"\n",
    "INDEX_PATH = DATA_DIR / \"matchings_index.json\"\n",
    "\n",
    "# Embeddings\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    MODEL = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "    def embed_vec(text: str) -> List[float]:\n",
    "        return MODEL.encode([text], normalize_embeddings=True)[0].tolist()\n",
    "    print(\"Usando sentence-transformers/all-MiniLM-L6-v2\")\n",
    "except Exception as e:\n",
    "    print(\"Fallback hash BOW 256D. Motivo:\", e)\n",
    "    VECTOR_SIZE = 256\n",
    "    def _hash_token(t: str) -> int:\n",
    "        h = 0\n",
    "        for ch in t:\n",
    "            h = (h << 5) - h + ord(ch)\n",
    "            h &= 0xFFFFFFFF\n",
    "        return abs(h)\n",
    "    def embed_vec(text: str) -> List[float]:\n",
    "        vec = [0.0]*VECTOR_SIZE\n",
    "        for t in text.lower().split():\n",
    "            idx = _hash_token(t) % VECTOR_SIZE\n",
    "            vec[idx]+=1.0\n",
    "        norm = math.sqrt(sum(v*v for v in vec)) or 1.0\n",
    "        return [v/norm for v in vec]\n",
    "\n",
    "def cosine(a: List[float], b: List[float]) -> float:\n",
    "    return float(sum(x*y for x,y in zip(a,b)))\n",
    "\n",
    "def read_jsonl(path: Path) -> List[Dict[str, Any]]:\n",
    "    rows=[]\n",
    "    if not path.exists():\n",
    "        return rows\n",
    "    with path.open() as f:\n",
    "        for line in f:\n",
    "            line=line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            try:\n",
    "                rows.append(json.loads(line))\n",
    "            except Exception:\n",
    "                continue\n",
    "    return rows\n",
    "\n",
    "def write_jsonl(path: Path, rows: List[Dict[str, Any]]):\n",
    "    with path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        for r in rows:\n",
    "            f.write(json.dumps(r, ensure_ascii=False)+\"\\n\")\n",
    "\n",
    "def build_text(p: Dict[str, Any]) -> str:\n",
    "    parts = [\n",
    "        p.get(\"title\", \"\"),\n",
    "        p.get(\"brand\", \"\"),\n",
    "        p.get(\"category_path\", \"\"),\n",
    "        p.get(\"category_path\", \"\"),\n",
    "        p.get(\"brand\", \"\"),\n",
    "    ]\n",
    "    desc = p.get(\"description\", \"\")\n",
    "    if desc:\n",
    "        parts.append(desc[:300])\n",
    "    return \" \".join(parts)\n",
    "\n",
    "# Load base data\n",
    "a = read_jsonl(PROD_PATH)\n",
    "pairs_gold = read_jsonl(PAIR_PATH)\n",
    "by_id = {p[\"id\"]: p for p in a}\n",
    "clients = [p for p in a if p.get(\"source\") == \"client\"]\n",
    "competitors = [p for p in a if p.get(\"source\") == \"competitor\"]\n",
    "\n",
    "# Embeddings matrix\n",
    "ids = [p[\"id\"] for p in a]\n",
    "embs = [embed_vec(build_text(p)) for p in a]\n",
    "embs_np = np.array(embs, dtype=np.float32)\n",
    "np.save(EMB_PATH, embs_np)\n",
    "with INDEX_PATH.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\"ids\": ids}, f, ensure_ascii=False)\n",
    "print(f\"Embeddings guardados: {EMB_PATH}\")\n",
    "\n",
    "idx_map = {pid:i for i,pid in enumerate(ids)}\n",
    "\n",
    "# Generar 1 distractor por gold\n",
    "random.seed(42)\n",
    "new_pairs = []\n",
    "next_pair_id = max((p.get(\"pair_id\",0) for p in pairs_gold), default=0)+1\n",
    "\n",
    "for pair in pairs_gold:\n",
    "    pair[\"label\"] = 1\n",
    "    pair[\"is_distractor\"] = False\n",
    "    new_pairs.append(pair)\n",
    "    client_id = pair[\"client_id\"]\n",
    "    used = {pair[\"competitor_id\"]}\n",
    "    client_cat = (by_id.get(client_id,{}).get(\"category_path\") or \"\").split(\">\")[0].strip().lower()\n",
    "    pool_all = [c for c in competitors if c[\"id\"] not in used]\n",
    "    pool_diff = [c for c in pool_all if (c.get(\"category_path\") or \"\").split(\">\")[0].strip().lower() != client_cat]\n",
    "    pool_use = pool_diff if pool_diff else pool_all\n",
    "    if not pool_use:\n",
    "        continue\n",
    "    neg = random.choice(pool_use)\n",
    "    new_pairs.append({\n",
    "        \"pair_id\": next_pair_id,\n",
    "        \"client_id\": client_id,\n",
    "        \"competitor_id\": neg[\"id\"],\n",
    "        \"label\": 0,\n",
    "        \"is_distractor\": True,\n",
    "    })\n",
    "    next_pair_id += 1\n",
    "\n",
    "# Score coseno\n",
    "scored = []\n",
    "for pair in new_pairs:\n",
    "    ci = idx_map.get(pair[\"client_id\"])\n",
    "    cj = idx_map.get(pair[\"competitor_id\"])\n",
    "    if ci is None or cj is None:\n",
    "        score = 0.0\n",
    "    else:\n",
    "        score = float(np.dot(embs_np[ci], embs_np[cj]))\n",
    "    pair[\"score\"] = score\n",
    "    pair[\"similarity\"] = score*100\n",
    "    scored.append(pair)\n",
    "\n",
    "write_jsonl(PAIR_PATH, scored)\n",
    "print(f\"Pares escritos en {PAIR_PATH}\")\n",
    "\n",
    "# Validación\n",
    "pos = [p[\"score\"] for p in scored if p.get(\"label\") == 1]\n",
    "neg = [p[\"score\"] for p in scored if p.get(\"label\") == 0]\n",
    "mean_pos = sum(pos)/max(len(pos),1)\n",
    "mean_neg = sum(neg)/max(len(neg),1)\n",
    "print(f\"Mean score label=1: {mean_pos:.4f} | label=0: {mean_neg:.4f}\")\n",
    "\n",
    "# Caso Garmin\n",
    "cl = next((p for p in clients if \"garmin quatix 7x solar watch\".lower() in p.get(\"title\",\"\" ).lower()), None)\n",
    "if cl:\n",
    "    print(\"Caso Garmin:\")\n",
    "    for pair in scored:\n",
    "        if pair[\"client_id\"] != cl[\"id\"]:\n",
    "            continue\n",
    "        print(f\"pair {pair['pair_id']} label={pair['label']} score={pair['score']:.4f} comp={pair['competitor_id']}\")\n",
    "\n",
    "print(\"Listo. Recarga la API con ?nocache=1\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env_tfm_product_matching (3.12.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}