{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a57a9b63",
   "metadata": {},
   "source": [
    "# 4_smart_connections_embed\n",
    "\n",
    "Calcula embeddings textuales para los productos de Smart Connections y escribe la similitud cliente↔competidor en los ficheros finales.\n",
    "\n",
    "Flujo:\n",
    "1. Lee `data/matchings_products.jsonl` y `data/matchings_pairs.jsonl` generados en el paso 4 (gold) o 4b (ruido).\n",
    "2. Calcula un embedding textual (Sentence-Transformer si está disponible; si no, hash BOW 256D).\n",
    "3. Para cada par cliente–competidor calcula coseno y lo guarda en `similarity` (0–100) y `score` (0–1).\n",
    "4. Sobrescribe los ficheros de salida con los nuevos campos.\n",
    "\n",
    "Ejecuta este notebook tras 4 o 4b para mejorar los scores en `/similar` sin cambiar la API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe0bde0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando sentence-transformers/all-MiniLM-L6-v2\n",
      "OK. Escritos 2074 productos y 8000 pares con similarity.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "import json, math, re\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "try:\n",
    "    PROJECT_ROOT = Path(__file__).resolve().parents[1]\n",
    "except NameError:\n",
    "    PROJECT_ROOT = Path.cwd().resolve().parents[0]\n",
    "    if not (PROJECT_ROOT / \"data\").exists() and len(PROJECT_ROOT.parents) > 0:\n",
    "        PROJECT_ROOT = PROJECT_ROOT.parent\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "PROD_PATH = DATA_DIR / \"matchings_products.jsonl\"\n",
    "PAIR_PATH = DATA_DIR / \"matchings_pairs.jsonl\"\n",
    "\n",
    "# Intenta usar sentence-transformers si está instalado; si no, fallback hash BOW\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    MODEL = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    def embed(text: str):\n",
    "        return MODEL.encode([text], normalize_embeddings=True)[0]\n",
    "    print(\"Usando sentence-transformers/all-MiniLM-L6-v2\")\n",
    "except Exception as e:\n",
    "    print(\"No sentence-transformers; usando hash BOW 256D\", e)\n",
    "    VECTOR_SIZE = 256\n",
    "    def hash_token(t: str) -> int:\n",
    "        h = 0\n",
    "        for ch in t:\n",
    "            h = (h << 5) - h + ord(ch)\n",
    "            h &= 0xFFFFFFFF\n",
    "        return abs(h)\n",
    "    def embed(text: str):\n",
    "        vec = [0.0]*VECTOR_SIZE\n",
    "        tokens = re.sub(r\"[^\\w\\s]+\", \" \", text.lower()).split()\n",
    "        for t in tokens:\n",
    "            idx = hash_token(t)%VECTOR_SIZE\n",
    "            vec[idx]+=1.0\n",
    "        norm = math.sqrt(sum(v*v for v in vec)) or 1.0\n",
    "        return [v/norm for v in vec]\n",
    "\n",
    "def cosine(a, b):\n",
    "    return float(sum(x*y for x,y in zip(a,b)))\n",
    "\n",
    "def read_jsonl(path: Path):\n",
    "    rows=[]\n",
    "    if not path.exists():\n",
    "        return rows\n",
    "    with path.open() as f:\n",
    "        for line in f:\n",
    "            line=line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            try:\n",
    "                rows.append(json.loads(line))\n",
    "            except Exception:\n",
    "                continue\n",
    "    return rows\n",
    "\n",
    "def write_jsonl(path: Path, rows):\n",
    "    with path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        for r in rows:\n",
    "            f.write(json.dumps(r, ensure_ascii=False)+\"\\n\")\n",
    "\n",
    "products = read_jsonl(PROD_PATH)\n",
    "pairs = read_jsonl(PAIR_PATH)\n",
    "\n",
    "by_id = {p[\"id\"]: p for p in products}\n",
    "\n",
    "# Precalcula embeddings por id\n",
    "emb_cache: Dict[str, List[float]] = {}\n",
    "for p in products:\n",
    "    text = f\"{p.get('title','')} {p.get('description','')}\"\n",
    "    emb_cache[p[\"id\"]] = embed(text)\n",
    "\n",
    "updated_pairs = []\n",
    "for pair in pairs:\n",
    "    c = by_id.get(pair[\"client_id\"])\n",
    "    comp = by_id.get(pair[\"competitor_id\"])\n",
    "    if not c or not comp:\n",
    "        continue\n",
    "    sim = cosine(emb_cache[c[\"id\"]], emb_cache[comp[\"id\"]])\n",
    "    comp_rec = by_id[comp[\"id\"]]\n",
    "    comp_rec[\"score\"] = sim\n",
    "    comp_rec[\"similarity\"] = sim*100\n",
    "    updated_pairs.append(pair)\n",
    "\n",
    "# reconstruimos la lista de productos con los campos actualizados\n",
    "updated_products = list(by_id.values())\n",
    "\n",
    "write_jsonl(PROD_PATH, updated_products)\n",
    "write_jsonl(PAIR_PATH, updated_pairs)\n",
    "print(f\"OK. Escritos {len(updated_products)} productos y {len(updated_pairs)} pares con similarity.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env_tfm_product_matching (3.12.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
