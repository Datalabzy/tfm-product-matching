{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6_embeddings_matchings\n",
    "\n",
    "Genera embeddings textuales para Smart Connections y recalcula los scores.\n",
    "\n",
    "Flujo:\n",
    "1. Lee `data/matchings_products.jsonl` y `data/matchings_pairs.jsonl`.\n",
    "2. Construye el texto como en el recomendador: `title + brand + category_path + category_path + brand + desc[:300]`.\n",
    "3. Embeddings con `all-MiniLM-L6-v2` (Sentence-Transformers); fallback hash BOW 256D si el modelo no carga.\n",
    "4. Guarda embeddings en `data/matchings_text.npy` y el índice en `data/matchings_index.json`.\n",
    "5. Recalcula `score` (coseno) y `similarity` para cada par y sobrescribe `data/matchings_pairs.jsonl`.\n",
    "\n",
    "Nota: los NPY no se versionan; siguen el mismo criterio que los pasos 2/3 del pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "cannot assign to expression here. Maybe you meant '==' instead of '='? (355548542.py, line 14)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31m+DATA_DIR = PROJECT_ROOT / \"data\"\u001b[39m\n    ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m cannot assign to expression here. Maybe you meant '==' instead of '='?\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "import json, math\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Any\n",
    "import numpy as np\n",
    "\n",
    "try:\n",
    "    PROJECT_ROOT = Path(__file__).resolve().parents[1]\n",
    "except NameError:\n",
    "    PROJECT_ROOT = Path.cwd().resolve().parents[0]\n",
    "    if not (PROJECT_ROOT / \"data\").exists() and len(PROJECT_ROOT.parents) > 0:\n",
    "        PROJECT_ROOT = PROJECT_ROOT.parent\n",
    "\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "PROD_PATH = DATA_DIR / \"matchings_products.jsonl\"\n",
    "PAIR_PATH = DATA_DIR / \"matchings_pairs.jsonl\"\n",
    "EMB_PATH = DATA_DIR / \"matchings_text.npy\"\n",
    "INDEX_PATH = DATA_DIR / \"matchings_index.json\"\n",
    "\n",
    "# --- Embedding helpers ---\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    MODEL = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "    def embed_vec(text: str) -> List[float]:\n",
    "        return MODEL.encode([text], normalize_embeddings=True)[0].tolist()\n",
    "    print(\"Usando sentence-transformers/all-MiniLM-L6-v2\")\n",
    "except Exception as e:\n",
    "    print(\"No sentence-transformers; fallback hash BOW 256D. Motivo:\", e)\n",
    "    VECTOR_SIZE = 256\n",
    "    def _hash_token(t: str) -> int:\n",
    "        h = 0\n",
    "        for ch in t:\n",
    "            h = (h << 5) - h + ord(ch)\n",
    "            h &= 0xFFFFFFFF\n",
    "        return abs(h)\n",
    "    def embed_vec(text: str) -> List[float]:\n",
    "        vec = [0.0]*VECTOR_SIZE\n",
    "        tokens = text.lower().split()\n",
    "        for t in tokens:\n",
    "            idx = _hash_token(t) % VECTOR_SIZE\n",
    "            vec[idx] += 1.0\n",
    "        norm = math.sqrt(sum(v*v for v in vec)) or 1.0\n",
    "        return [v/norm for v in vec]\n",
    "\n",
    "def cosine(a: List[float], b: List[float]) -> float:\n",
    "    return float(sum(x*y for x,y in zip(a,b)))\n",
    "\n",
    "def read_jsonl(path: Path) -> List[Dict[str, Any]]:\n",
    "    rows=[]\n",
    "    if not path.exists():\n",
    "        return rows\n",
    "    with path.open() as f:\n",
    "        for line in f:\n",
    "            line=line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            try:\n",
    "                rows.append(json.loads(line))\n",
    "            except Exception:\n",
    "                continue\n",
    "    return rows\n",
    "\n",
    "def write_jsonl(path: Path, rows: List[Dict[str, Any]]):\n",
    "    with path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        for r in rows:\n",
    "            f.write(json.dumps(r, ensure_ascii=False)+\"\\n\")\n",
    "\n",
    "def build_text(p: Dict[str, Any]) -> str:\n",
    "    parts = [\n",
    "        p.get(\"title\", \"\"),\n",
    "        p.get(\"brand\", \"\"),\n",
    "        p.get(\"category_path\", \"\"),\n",
    "        p.get(\"category_path\", \"\"),\n",
    "        p.get(\"brand\", \"\"),\n",
    "    ]\n",
    "    desc = p.get(\"description\", \"\")\n",
    "    if desc:\n",
    "        parts.append(desc[:300])\n",
    "    return \" \".join(parts)\n",
    "\n",
    "# --- Load data ---\n",
    "products = read_jsonl(PROD_PATH)\n",
    "pairs = read_jsonl(PAIR_PATH)\n",
    "by_id = {p[\"id\"]: p for p in products}\n",
    "\n",
    "# --- Embeddings matrix ---\n",
    "embs = []\n",
    "ids = []\n",
    "for p in products:\n",
    "    ids.append(p[\"id\"])\n",
    "    embs.append(embed_vec(build_text(p)))\n",
    "embs_np = np.array(embs, dtype=np.float32)\n",
    "np.save(EMB_PATH, embs_np)\n",
    "with INDEX_PATH.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\"ids\": ids}, f, ensure_ascii=False)\n",
    "print(f\"Embeddings guardados en {EMB_PATH}, index en {INDEX_PATH}\")\n",
    "\n",
    "# --- Score pairs ---\n",
    "idx_map = {pid: i for i, pid in enumerate(ids)}\n",
    "scored_pairs = []\n",
    "for pair in pairs:\n",
    "    ci = idx_map.get(pair[\"client_id\"])\n",
    "    cj = idx_map.get(pair[\"competitor_id\"])\n",
    "    if ci is None or cj is None:\n",
    "        score = 0.0\n",
    "    else:\n",
    "        score = float(np.dot(embs_np[ci], embs_np[cj]))\n",
    "    pair[\"score\"] = score\n",
    "    pair[\"similarity\"] = score * 100\n",
    "    if \"label\" not in pair:\n",
    "        pair[\"label\"] = 1  # por si faltara\n",
    "    if \"is_distractor\" not in pair:\n",
    "        pair[\"is_distractor\"] = pair.get(\"label\",1)==0\n",
    "    scored_pairs.append(pair)\n",
    "\n",
    "write_jsonl(PAIR_PATH, scored_pairs)\n",
    "print(f\"Pares recalculados y escritos en {PAIR_PATH}\")\n",
    "\n",
    "# --- Validación ---\n",
    "positives = [p[\"score\"] for p in scored_pairs if p.get(\"label\") == 1]\n",
    "negatives = [p[\"score\"] for p in scored_pairs if p.get(\"label\") == 0]\n",
    "mean_pos = sum(positives) / max(len(positives), 1)\n",
    "mean_neg = sum(negatives) / max(len(negatives), 1)\n",
    "print(f\"Mean score label=1: {mean_pos:.4f} | label=0: {mean_neg:.4f}\")\n",
    "\n",
    "# Caso Garmin\n",
    "garmin = next((p for p in products if \"garmin quatix 7x solar watch\".lower() in p.get(\"title\",\"\" ).lower()), None)\n",
    "if garmin:\n",
    "    print(\"\\nCaso Garmin:\")\n",
    "    for pair in scored_pairs:\n",
    "        if pair[\"client_id\"] != garmin[\"id\"]:\n",
    "            continue\n",
    "        print(f\"pair {pair['pair_id']} label={pair['label']} score={pair['score']:.4f} comp={pair['competitor_id']}\")\n",
    "\n",
    "print(\"Listo. Recarga la API con ?nocache=1 tras ejecutar este notebook.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env_tfm_product_matching (3.12.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
