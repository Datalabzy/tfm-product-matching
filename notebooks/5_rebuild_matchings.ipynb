{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c0f8339",
   "metadata": {},
   "source": [
    "# 5_rebuild_matchings\n",
    "\n",
    "Reconstruye el dataset de Smart Connections con etiquetas y scores coherentes.\n",
    "\n",
    "Pasos:\n",
    "1. Lee `data/matchings_products.jsonl` y `data/matchings_pairs.jsonl` (productos no se tocan).\n",
    "2. Genera un negativo (ruido) por cada par positivo.\n",
    "3. Calcula embeddings de texto con el mismo pipeline del recomendador (MiniLM; fallback hash BOW).\n",
    "4. Score = coseno; similarity = score*100.\n",
    "5. Valida medias label=1 vs label=0 y muestra el caso Garmin.\n",
    "\n",
    "Salida: `data/matchings_pairs_scored.jsonl`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325ec074",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 93) (3665607542.py, line 93)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 93\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mf.write(json.dumps(r, ensure_ascii=False) + \"\u001b[39m\n                                                ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m unterminated string literal (detected at line 93)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Reconstruye el dataset de Smart Connections con labels y scores coherentes.\n",
    "\n",
    "Pasos implementados:\n",
    "1) Lee products.jsonl y pairs.jsonl (sin tocarlos) desde data/.\n",
    "2) Genera negativos (ruido) por cada positivo.\n",
    "3) Calcula embeddings de texto con el MISMO pipeline del recomendador:\n",
    "   text = title + brand + category_path + desc[:300]\n",
    "   modelo: all-MiniLM-L6-v2 (sentence-transformers); fallback hash BOW 256D.\n",
    "4) Score = coseno(emb_client, emb_comp); similarity = score*100.\n",
    "5) Valida medias label=1 vs label=0 y muestra caso Garmin.\n",
    "\n",
    "Salida:\n",
    "  data/matchings_products.jsonl      (se copia tal cual, no se modifica)\n",
    "  data/matchings_pairs.jsonl         (pares con label/score/similarity)\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import math\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Any\n",
    "\n",
    "try:\n",
    "    PROJECT_ROOT = Path(__file__).resolve().parents[1]\n",
    "except NameError:\n",
    "    PROJECT_ROOT = Path.cwd().resolve().parents[0]\n",
    "    if not (PROJECT_ROOT / \"data\").exists() and len(PROJECT_ROOT.parents) > 0:\n",
    "        PROJECT_ROOT = PROJECT_ROOT.parent\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "\n",
    "PROD_IN = DATA_DIR / \"matchings_products.jsonl\"\n",
    "PAIR_IN = DATA_DIR / \"matchings_pairs.jsonl\"\n",
    "PAIR_OUT = DATA_DIR / \"matchings_pairs.jsonl\"\n",
    "\n",
    "# ---------- Embedding utils ----------\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "\n",
    "    MODEL = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "    def embed_vec(text: str) -> List[float]:\n",
    "        return MODEL.encode([text], normalize_embeddings=True)[0].tolist()\n",
    "\n",
    "    print(\"Usando sentence-transformers/all-MiniLM-L6-v2\")\n",
    "except Exception as e:\n",
    "    print(\"No sentence-transformers, usando hash BOW 256D. Motivo:\", e)\n",
    "    VECTOR_SIZE = 256\n",
    "\n",
    "    def _hash_token(t: str) -> int:\n",
    "        h = 0\n",
    "        for ch in t:\n",
    "            h = (h << 5) - h + ord(ch)\n",
    "            h &= 0xFFFFFFFF\n",
    "        return abs(h)\n",
    "\n",
    "    def embed_vec(text: str) -> List[float]:\n",
    "        vec = [0.0] * VECTOR_SIZE\n",
    "        tokens = text.lower().split()\n",
    "        for t in tokens:\n",
    "            idx = _hash_token(t) % VECTOR_SIZE\n",
    "            vec[idx] += 1.0\n",
    "        norm = math.sqrt(sum(v * v for v in vec)) or 1.0\n",
    "        return [v / norm for v in vec]\n",
    "\n",
    "\n",
    "def cosine(a: List[float], b: List[float]) -> float:\n",
    "    return float(sum(x * y for x, y in zip(a, b)))\n",
    "\n",
    "\n",
    "# ---------- IO helpers ----------\n",
    "def read_jsonl(path: Path) -> List[Dict[str, Any]]:\n",
    "    rows: List[Dict[str, Any]] = []\n",
    "    if not path.exists():\n",
    "        return rows\n",
    "    with path.open() as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            try:\n",
    "                rows.append(json.loads(line))\n",
    "            except Exception:\n",
    "                continue\n",
    "    return rows\n",
    "\n",
    "\n",
    "def write_jsonl(path: Path, rows: List[Dict[str, Any]]) -> None:\n",
    "    with path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        for r in rows:\n",
    "            f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "\n",
    "# ---------- Main pipeline ----------\n",
    "def build_text(p: Dict[str, Any]) -> str:\n",
    "    parts = [\n",
    "        p.get(\"title\", \"\"),\n",
    "        p.get(\"brand\", \"\"),\n",
    "        p.get(\"category_path\", \"\"),\n",
    "        p.get(\"category_path\", \"\"),\n",
    "        p.get(\"brand\", \"\"),\n",
    "    ]\n",
    "    desc = p.get(\"description\", \"\")\n",
    "    if desc:\n",
    "        parts.append(desc[:300])\n",
    "    return \" \".join(parts)\n",
    "\n",
    "\n",
    "def main():\n",
    "    products = read_jsonl(PROD_IN)\n",
    "    pairs = read_jsonl(PAIR_IN)\n",
    "\n",
    "    by_id = {p[\"id\"]: p for p in products}\n",
    "    clients = [p for p in products if p.get(\"source\") == \"client\"]\n",
    "    competitors = [p for p in products if p.get(\"source\") == \"competitor\"]\n",
    "\n",
    "    # Embeddings por id\n",
    "    emb_cache: Dict[str, List[float]] = {}\n",
    "    for p in products:\n",
    "        emb_cache[p[\"id\"]] = embed_vec(build_text(p))\n",
    "\n",
    "    # Generar negativos\n",
    "    next_pair_id = max((p.get(\"pair_id\", 0) for p in pairs), default=0) + 1\n",
    "    random.seed(42)\n",
    "    new_pairs: List[Dict[str, Any]] = []\n",
    "\n",
    "    for pair in pairs:\n",
    "        pair[\"label\"] = 1\n",
    "        pair[\"is_distractor\"] = False\n",
    "        new_pairs.append(pair)\n",
    "\n",
    "        client_id = pair[\"client_id\"]\n",
    "        used = {pair[\"competitor_id\"]}\n",
    "        client_cat = (by_id.get(client_id, {}).get(\"category_path\") or \"\").split(\">\")[0].strip().lower()\n",
    "        pool_all = [c for c in competitors if c[\"id\"] not in used]\n",
    "        pool_diff = [c for c in pool_all if (c.get(\"category_path\") or \"\").split(\">\")[0].strip().lower() != client_cat]\n",
    "        pool_use = pool_diff if pool_diff else pool_all\n",
    "        if not pool_use:\n",
    "            continue\n",
    "        neg = random.choice(pool_use)\n",
    "        new_pairs.append(\n",
    "            {\n",
    "                \"pair_id\": next_pair_id,\n",
    "                \"client_id\": client_id,\n",
    "                \"competitor_id\": neg[\"id\"],\n",
    "                \"label\": 0,\n",
    "                \"is_distractor\": True,\n",
    "            }\n",
    "        )\n",
    "        next_pair_id += 1\n",
    "\n",
    "    # Calcular score y similarity\n",
    "    scored_pairs: List[Dict[str, Any]] = []\n",
    "    for pair in new_pairs:\n",
    "        c = emb_cache.get(pair[\"client_id\"])\n",
    "        comp = emb_cache.get(pair[\"competitor_id\"])\n",
    "        if c is None or comp is None:\n",
    "            score = 0.0\n",
    "        else:\n",
    "            score = cosine(c, comp)\n",
    "        pair[\"score\"] = score\n",
    "        pair[\"similarity\"] = score * 100\n",
    "        scored_pairs.append(pair)\n",
    "\n",
    "    # Validaci√≥n simple\n",
    "    positives = [p[\"score\"] for p in scored_pairs if p.get(\"label\") == 1]\n",
    "    negatives = [p[\"score\"] for p in scored_pairs if p.get(\"label\") == 0]\n",
    "    mean_pos = sum(positives) / max(len(positives), 1)\n",
    "    mean_neg = sum(negatives) / max(len(negatives), 1)\n",
    "    print(f\"Mean score (label=1): {mean_pos:.4f}\")\n",
    "    print(f\"Mean score (label=0): {mean_neg:.4f}\")\n",
    "\n",
    "    # Caso Garmin\n",
    "    garmin_client = next((p for p in clients if \"garmin quatix 7x solar watch\".lower() in p.get(\"title\", \"\").lower()), None)\n",
    "    if garmin_client:\n",
    "        print(\"\\nCaso Garmin Quatix 7X Solar watch\")\n",
    "        for pair in scored_pairs:\n",
    "            if pair[\"client_id\"] != garmin_client[\"id\"]:\n",
    "                continue\n",
    "            print(\n",
    "                f\"pair_id={pair['pair_id']} label={pair['label']} score={pair['score']:.4f} comp={pair['competitor_id']}\"\n",
    "            )\n",
    "\n",
    "    write_jsonl(PAIR_OUT, scored_pairs)\n",
    "    print(f\"Escritos {len(scored_pairs)} pares en {PAIR_OUT}\")\n",
    "    print(\"Products no se tocan; la UI puede leer score/similarity directamente de matchings_pairs.jsonl\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env_tfm_product_matching (3.12.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
